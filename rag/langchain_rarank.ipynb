{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7140f392-4690-4553-a1bb-c8cc3119a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "# Import the FlashRank-specific compressor\n",
    "from langchain_community.document_compressors import FlashrankRerank\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "MINIML_MODEL_PATH = os.getenv(\"MINIML_MODEL_PATH\")\n",
    "\n",
    "# Sample text\n",
    "sample_text = '''Artificial Intelligence refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.\n",
    "        AI is used in fields such as healthcare (diagnostic tools), finance (fraud detection), and customer service (chatbots).\n",
    "        Key types include Narrow AI (focused on specific tasks like voice assistants) and General AI (hypothetical AI capable of performing any intellectual task a human can do).\n",
    "        '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b013c5ed-936e-4967-ad77-cfc9f3bc709a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3 chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d83b19fd2941639afaff1389377aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Create Document objects\n",
    "doc_object = Document(page_content=sample_text, metadata={\"source\": \"manual_input\"})\n",
    "\n",
    "# 2. Chunk the text\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)\n",
    "chunks = text_splitter.split_documents([doc_object])\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "# 3. Initialize Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=MINIML_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf92aa2-f3f2-4d15-b275-4bda38bb1200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3 chunks\n",
      "Chunk 1: Artificial Intelligence refers to the simulation of human intelligence in machines that are programm...\n",
      "Chunk 2: AI is used in fields such as healthcare (diagnostic tools), finance (fraud detection), and customer ...\n",
      "Chunk 3: Key types include Narrow AI (focused on specific tasks like voice assistants) and General AI (hypoth...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e9ff085d2942f69e1cfe48843b250e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Chroma vector database...\n",
      "Chroma vector database created and stored locally.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.documents import Document\n",
    "from flashrank import Ranker, RerankRequest\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "MINIML_MODEL_PATH = os.getenv(\"MINIML_MODEL_PATH\")\n",
    "\n",
    "# Sample text\n",
    "sample_text = '''Artificial Intelligence refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.\n",
    "        AI is used in fields such as healthcare (diagnostic tools), finance (fraud detection), and customer service (chatbots).\n",
    "        Key types include Narrow AI (focused on specific tasks like voice assistants) and General AI (hypothetical AI capable of performing any intellectual task a human can do).\n",
    "        '''\n",
    "\n",
    "# 1. Create Document objects\n",
    "doc_object = Document(page_content=sample_text, metadata={\"source\": \"manual_input\"})\n",
    "\n",
    "# 2. Chunk the text\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)\n",
    "chunks = text_splitter.split_documents([doc_object])\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk.page_content[:100]}...\")\n",
    "\n",
    "# 3. Initialize Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=MINIML_MODEL_PATH)\n",
    "\n",
    "# 4. Create persist directory\n",
    "current_dir = os.getcwd()\n",
    "persist_directory = os.path.join(current_dir, \"resources\", \"databases\", \"db1\", \"chroma_db\")\n",
    "os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "# 5. Create Chroma Vector DB\n",
    "print(\"\\nCreating Chroma vector database...\")\n",
    "chroma_db = Chroma.from_documents(\n",
    "    documents=chunks, \n",
    "    embedding=embeddings, \n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "print(\"Chroma vector database created and stored locally.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cadd4d3-b4f1-47e8-8f3f-3a7303cce9b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for FlashrankRerank\nranker\n  Extra inputs are not permitted [type=extra_forbidden, input_value=<flashrank.Ranker.Ranker object at 0x7acb7ff51270>, input_type=Ranker]\n    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\nscore\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m ranker \u001b[38;5;241m=\u001b[39m Ranker(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mms-marco-MiniLM-L-12-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Now create the FlashrankRerank compressor\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m compressor \u001b[38;5;241m=\u001b[39m \u001b[43mFlashrankRerank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mranker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mranker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the initialized ranker\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Alternative: Let FlashrankRerank initialize the ranker for you\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# (This should also work after model_rebuild)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Try the simple initialization first\u001b[39;00m\n",
      "File \u001b[0;32m/media/scientist/volume/mr_document/Linux_Git/AgenticAI/newagentic_venv/lib/python3.10/site-packages/pydantic/main.py:250\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    249\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    252\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    256\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    257\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for FlashrankRerank\nranker\n  Extra inputs are not permitted [type=extra_forbidden, input_value=<flashrank.Ranker.Ranker object at 0x7acb7ff51270>, input_type=Ranker]\n    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden\nscore\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "# First, import the Ranker from flashrank\n",
    "from flashrank import Ranker, RerankRequest\n",
    "\n",
    "# Initialize the Ranker\n",
    "ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "# Now create the FlashrankRerank compressor\n",
    "compressor = FlashrankRerank(\n",
    "    ranker=ranker,  # Pass the initialized ranker\n",
    "    top_n=3,\n",
    "    score=True\n",
    ")\n",
    "\n",
    "# Alternative: Let FlashrankRerank initialize the ranker for you\n",
    "# (This should also work after model_rebuild)\n",
    "try:\n",
    "    # Try the simple initialization first\n",
    "    compressor = FlashrankRerank(\n",
    "        model=\"ms-marco-MiniLM-L-12-v2\",\n",
    "        top_n=3,\n",
    "        score=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Simple initialization failed: {e}\")\n",
    "    print(\"Using explicit ranker initialization...\")\n",
    "    \n",
    "    # If that fails, initialize with explicit ranker\n",
    "    from flashrank import Ranker\n",
    "    ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\")\n",
    "    compressor = FlashrankRerank(\n",
    "        ranker=ranker,\n",
    "        top_n=3,\n",
    "        score=True\n",
    "    )\n",
    "\n",
    "# Create base retriever (retrieve more documents for reranking)\n",
    "base_retriever = chroma_db.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# Create compression retriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "# Test the reranker\n",
    "print(f\"Query: {query}\")\n",
    "reranked_docs = compression_retriever.invoke(query)  # Use invoke() instead of get_relevant_documents()\n",
    "\n",
    "print(f\"\\nAfter FlashrankRerank (top 3 results):\")\n",
    "for i, doc in enumerate(reranked_docs):\n",
    "    print(f\"Result {i+1}: {doc.page_content[:150]}...\")\n",
    "    if hasattr(doc, 'metadata'):\n",
    "        print(f\"  Metadata: {doc.metadata}\\n\")\n",
    "\n",
    "# 8. Alternative: Direct FlashRank implementation (simpler, no Pydantic issues)\n",
    "print(f\"\\n--- Alternative: Direct FlashRank Implementation ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb25ea18-c413-4252-92ab-96ae660afb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrank import Ranker, RerankRequest\n",
    "\n",
    "# Initialize FlashRank directly\n",
    "direct_ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "def rerank_with_flashrank(query, documents, top_k=3):\n",
    "    \"\"\"Direct reranking without LangChain wrapper\"\"\"\n",
    "    \n",
    "    # Prepare passages for FlashRank\n",
    "    passages = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        passages.append({\n",
    "            \"id\": i,\n",
    "            \"text\": doc.page_content,\n",
    "            \"metadata\": doc.metadata\n",
    "        })\n",
    "    \n",
    "    # Create rerank request\n",
    "    rerank_request = RerankRequest(query=query, passages=passages)\n",
    "    \n",
    "    # Perform reranking\n",
    "    results = direct_ranker.rerank(rerank_request)\n",
    "    \n",
    "    # Get top_k results\n",
    "    top_results = results[:top_k]\n",
    "    \n",
    "    # Return reranked documents with scores\n",
    "    reranked_docs = []\n",
    "    for result in top_results:\n",
    "        original_idx = result[\"id\"]\n",
    "        doc = documents[original_idx]\n",
    "        # Add score to metadata\n",
    "        doc.metadata[\"relevance_score\"] = result[\"score\"]\n",
    "        doc.metadata[\"rank\"] = result.get(\"index\", 0)\n",
    "        reranked_docs.append(doc)\n",
    "    \n",
    "    return reranked_docs\n",
    "\n",
    "# Get initial documents\n",
    "initial_docs = chroma_db.similarity_search(query, k=10)\n",
    "print(f\"Retrieved {len(initial_docs)} initial documents\")\n",
    "\n",
    "# Apply direct reranking\n",
    "direct_results = rerank_with_flashrank(query, initial_docs, top_k=3)\n",
    "\n",
    "print(f\"\\nDirect FlashRank results:\")\n",
    "for i, doc in enumerate(direct_results):\n",
    "    score = doc.metadata.get('relevance_score', 'N/A')\n",
    "    print(f\"Result {i+1} (Score: {score:.4f}): {doc.page_content[:150]}...\")\n",
    "\n",
    "# 9. Comparison function\n",
    "print(f\"\\n--- Comparison: Without vs With Reranking ---\")\n",
    "\n",
    "def compare_methods(query, k=3):\n",
    "    print(f\"Query: '{query}'\")\n",
    "    \n",
    "    # Method 1: Without reranking\n",
    "    without = chroma_db.similarity_search(query, k=k)\n",
    "    print(f\"\\n1. WITHOUT RERANKING (Similarity Search):\")\n",
    "    for i, doc in enumerate(without):\n",
    "        print(f\"   {i+1}. {doc.page_content[:100]}...\")\n",
    "    \n",
    "    # Method 2: With direct FlashRank reranking\n",
    "    initial = chroma_db.similarity_search(query, k=10)\n",
    "    with_rerank = rerank_with_flashrank(query, initial, top_k=k)\n",
    "    print(f\"\\n2. WITH FLASHRANK RERANKING:\")\n",
    "    for i, doc in enumerate(with_rerank):\n",
    "        score = doc.metadata.get('relevance_score', 'N/A')\n",
    "        print(f\"   {i+1}. Score: {score:.4f} - {doc.page_content[:100]}...\")\n",
    "\n",
    "# Run comparison\n",
    "compare_methods(query)\n",
    "\n",
    "# 10. Test with multiple queries\n",
    "print(f\"\\n--- Testing Multiple Queries ---\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What are the applications of AI in healthcare?\",\n",
    "    \"Explain Narrow AI and General AI\",\n",
    "    \"How do machines mimic human intelligence?\",\n",
    "]\n",
    "\n",
    "for test_query in test_queries:\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Query: '{test_query}'\")\n",
    "    \n",
    "    # Get initial results\n",
    "    initial = chroma_db.similarity_search(test_query, k=8)\n",
    "    \n",
    "    # Rerank\n",
    "    reranked = rerank_with_flashrank(test_query, initial, top_k=2)\n",
    "    \n",
    "    print(\"Top 2 reranked results:\")\n",
    "    for i, doc in enumerate(reranked):\n",
    "        score = doc.metadata.get('relevance_score', 'N/A')\n",
    "        print(f\"{i+1}. (Score: {score:.4f}) {doc.page_content[:120]}...\")\n",
    "\n",
    "# 11. Custom retriever class (using direct FlashRank)\n",
    "print(f\"\\n--- Custom Retriever with FlashRank ---\")\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing import List\n",
    "\n",
    "class FlashRankRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever with built-in FlashRank reranking\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, k_final=3, k_initial=10, model=\"ms-marco-MiniLM-L-12-v2\"):\n",
    "        super().__init__()\n",
    "        self.vectorstore = vectorstore\n",
    "        self.k_final = k_final\n",
    "        self.k_initial = k_initial\n",
    "        self.ranker = Ranker(model_name=model)\n",
    "    \n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Get initial documents\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=self.k_initial)\n",
    "        \n",
    "        # Prepare for reranking\n",
    "        passages = [{\"text\": doc.page_content, \"metadata\": doc.metadata} \n",
    "                   for doc in initial_docs]\n",
    "        \n",
    "        # Rerank\n",
    "        rerank_request = RerankRequest(query=query, passages=passages)\n",
    "        results = self.ranker.rerank(rerank_request)\n",
    "        \n",
    "        # Return top k_final documents\n",
    "        reranked_docs = []\n",
    "        for result in results[:self.k_final]:\n",
    "            doc = initial_docs[result[\"id\"]]\n",
    "            doc.metadata[\"relevance_score\"] = result[\"score\"]\n",
    "            reranked_docs.append(doc)\n",
    "        \n",
    "        return reranked_docs\n",
    "\n",
    "# Test custom retriever\n",
    "custom_retriever = FlashRankRetriever(\n",
    "    vectorstore=chroma_db,\n",
    "    k_final=2,\n",
    "    k_initial=8\n",
    ")\n",
    "\n",
    "print(f\"Testing custom retriever with query: '{query}'\")\n",
    "custom_results = custom_retriever._get_relevant_documents(query)\n",
    "for i, doc in enumerate(custom_results):\n",
    "    score = doc.metadata.get('relevance_score', 'N/A')\n",
    "    print(f\"Custom result {i+1} (Score: {score:.4f}): {doc.page_content[:100]}...\")\n",
    "\n",
    "print(\"\\nâœ… Implementation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newagentic_venv",
   "language": "python",
   "name": "newagentic_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
